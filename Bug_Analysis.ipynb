{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35a76e5e-3bb7-4cc7-aec0-00745262e953",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Apache JIRA Bug Analysis and Clustering\n",
    "\n",
    "This notebook performs comprehensive analysis of Apache JIRA bugs, including:\n",
    "1. Bug reopening prediction\n",
    "2. Bug clustering\n",
    "3. Feature importance analysis\n",
    "\n",
    "## Key Analysis Questions\n",
    "\n",
    "This analysis addresses the following key questions:\n",
    "\n",
    "1. **Bug Reopening Detection**\n",
    "   - What percentage of bugs are reopened after being resolved?\n",
    "   - Can we identify patterns in the changelog that indicate bug reopening?\n",
    "\n",
    "2. **Bug Reopening Predictive Factors**\n",
    "   - What features differentiate reopened bugs from non-reopened bugs?\n",
    "   - Which factors are most important in predicting whether a bug will be reopened?\n",
    "\n",
    "3. **Machine Learning Prediction**\n",
    "   - How accurately can we predict which bugs will be reopened?\n",
    "   - Which ML models perform best for bug reopening prediction?\n",
    "   - What evaluation metrics are most relevant for this prediction task?\n",
    "\n",
    "4. **Bug Clustering**\n",
    "   - What are the natural groupings of bugs based on their characteristics?\n",
    "   - How many distinct clusters exist in the bug dataset?\n",
    "   - What keywords and features characterize each cluster?\n",
    "\n",
    "5. **Cluster-Reopening Relationship**\n",
    "   - Do certain types of bugs (clusters) have higher reopening rates?\n",
    "   - What characteristics distinguish clusters with high vs. low reopening rates?\n",
    "\n",
    "6. **Practical Applications**\n",
    "   - How can these insights improve bug triage and resolution processes?\n",
    "   - What preventive measures could reduce bug reopening rates?\n",
    "\n",
    "## Overview\n",
    "\n",
    "This analysis consists of several key components:\n",
    "- Identification of bugs that were reopened after being resolved\n",
    "- Machine learning models to predict which bugs are likely to be reopened\n",
    "- Clustering of bugs based on their characteristics\n",
    "- Identification of common patterns within bug clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3ca7649-4166-4040-afb9-8417edc48603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Imports and Setup\n",
    "\n",
    "First, we import the necessary libraries and set up our Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1f2d8b8-5e2e-4585-aa19-788ab054a202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, expr, lit, count, collect_list, size, array_contains, \n",
    "    countDistinct, datediff, to_date, desc, regexp_replace, lower, \n",
    "    concat_ws, split, explode, array_join\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, ArrayType, BooleanType\n",
    "\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder, VectorAssembler, CountVectorizer,\n",
    "    IDF, Word2Vec, StopWordsRemover, Tokenizer, HashingTF, NGram\n",
    ")\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression, RandomForestClassifier, \n",
    "    GBTClassifier, DecisionTreeClassifier\n",
    ")\n",
    "from pyspark.ml.clustering import KMeans, BisectingKMeans\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, ClusteringEvaluator\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Apache JIRA Bug Analysis and Clustering\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b36cefc1-0ffd-409f-b504-4f11b0035974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "We load JIRA issue and changelog data from HDFS into Spark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "669509c0-965a-43f9-b981-cbde29e311a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File paths (update these to your actual paths)\n",
    "issues_path = \"/user/szreiqa/Apache_JIRA_Issues/cleaned_issues.parquet\"\n",
    "changelog_path = \"/user/szreiqa/Apache_JIRA_Issues/cleaned_changelog.parquet\"\n",
    "\n",
    "# Load issues data\n",
    "print(f\"Loading issues from: {issues_path}\")\n",
    "issues_df = spark.read.parquet(issues_path)\n",
    "issue_count = issues_df.count()\n",
    "print(f\"Total issues: {issue_count}\")\n",
    "\n",
    "# Filter for bugs only\n",
    "bugs_df = issues_df.filter(col(\"issuetype_name\") == \"Bug\")\n",
    "bug_count = bugs_df.count()\n",
    "print(f\"Total bugs: {bug_count} ({bug_count/issue_count*100:.2f}% of all issues)\")\n",
    "\n",
    "# Display schema to understand the data structure\n",
    "print(\"\\nBugs schema:\")\n",
    "bugs_df.printSchema()\n",
    "\n",
    "# Load changelog data\n",
    "print(f\"\\nLoading changelog from: {changelog_path}\")\n",
    "changelog_df = spark.read.parquet(changelog_path)\n",
    "changelog_count = changelog_df.count()\n",
    "print(f\"Total changelog entries: {changelog_count}\")\n",
    "\n",
    "# Display schema\n",
    "print(\"\\nChangelog schema:\")\n",
    "changelog_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0b339a7-7c1c-440a-ae89-469af9f8cb7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's examine a few sample bugs to understand the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6c0e1ae-cfd0-48e2-9ae6-f83a4c7d29ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display sample bugs\n",
    "bugs_df.select(\"key\", \"summary\", \"priority_name\", \"status_name\", \"created\", \"resolutiondate\").limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e49f1fa-4e67-441d-8379-9db0a30a72e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Bug Reopening Detection\n",
    "\n",
    "We'll analyze the changelog to identify bugs that were reopened after being resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5c000e5-dabf-44dd-9321-dedf2e0c8994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter status changes from the changelog\n",
    "status_changes = changelog_df.filter(\n",
    "    (col(\"field\") == \"status\") & \n",
    "    col(\"fromString\").isNotNull() & \n",
    "    col(\"toString\").isNotNull()\n",
    ")\n",
    "\n",
    "# Group changes by issue key and collect status transitions\n",
    "status_transitions = status_changes.groupBy(\"key\").agg(\n",
    "    collect_list(\"fromString\").alias(\"from_statuses\"),\n",
    "    collect_list(\"toString\").alias(\"to_statuses\")\n",
    ")\n",
    "\n",
    "# Define a function to detect reopening patterns\n",
    "def has_reopen_pattern(from_statuses, to_statuses):\n",
    "    \"\"\"Detects if the issue has been reopened based on status transitions.\"\"\"\n",
    "    if not from_statuses or not to_statuses or len(from_statuses) != len(to_statuses):\n",
    "        return False\n",
    "        \n",
    "    # Define resolution and reopening statuses\n",
    "    resolution_statuses = [\"resolved\", \"closed\", \"done\", \"fixed\", \"completed\"]\n",
    "    reopen_statuses = [\"reopened\", \"in progress\", \"open\", \"todo\", \"to do\", \"in development\"]\n",
    "    \n",
    "    # Look for patterns where a resolved issue is reopened\n",
    "    for i in range(len(from_statuses) - 1):\n",
    "        current_to = to_statuses[i].lower()\n",
    "        next_from = from_statuses[i+1].lower()\n",
    "        next_to = to_statuses[i+1].lower()\n",
    "        \n",
    "        # Check if an issue moved to resolved/closed and then away from it\n",
    "        if any(status in current_to for status in resolution_statuses) and \\\n",
    "           any(status in next_from for status in resolution_statuses) and \\\n",
    "           any(status in next_to for status in reopen_statuses):\n",
    "            return True\n",
    "            \n",
    "    # Also check for direct reopened status\n",
    "    if any(status.lower() == \"reopened\" for status in to_statuses):\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "# Register UDF for use in Spark\n",
    "from pyspark.sql.functions import udf\n",
    "reopen_pattern_udf = udf(has_reopen_pattern, BooleanType())\n",
    "\n",
    "# Apply UDF to detect reopened issues\n",
    "bugs_with_reopen = status_transitions.withColumn(\n",
    "    \"was_reopened\", reopen_pattern_udf(col(\"from_statuses\"), col(\"to_statuses\"))\n",
    ")\n",
    "\n",
    "# Join with bugs dataframe\n",
    "bugs_with_reopen_flag = bugs_df.join(\n",
    "    bugs_with_reopen.select(\"key\", \"was_reopened\"),\n",
    "    \"key\",\n",
    "    \"left\"\n",
    ").withColumn(\n",
    "    \"was_reopened\", \n",
    "    when(col(\"was_reopened\").isNull(), False).otherwise(col(\"was_reopened\"))\n",
    ")\n",
    "\n",
    "# Count reopened bugs\n",
    "reopened_count = bugs_with_reopen_flag.filter(col(\"was_reopened\") == True).count()\n",
    "print(f\"Total bugs: {bug_count}\")\n",
    "print(f\"Bugs with reopening pattern: {reopened_count} ({reopened_count/bug_count*100:.2f}%)\")\n",
    "\n",
    "# Show some examples of reopened bugs\n",
    "print(\"\\nExamples of reopened bugs:\")\n",
    "bugs_with_reopen_flag.filter(col(\"was_reopened\") == True) \\\n",
    "    .select(\"key\", \"summary\", \"priority_name\", \"status_name\") \\\n",
    "    .limit(5) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ce38bad-521f-460c-95cc-91ebd7e98975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Feature Engineering for Bug Reopening Prediction\n",
    "\n",
    "Next, we'll prepare features for building machine learning models to predict which bugs are likely to be reopened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "353527f8-0385-4fa6-ba08-cc83112e4173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract project from issue key\n",
    "bugs_with_features = bugs_with_reopen_flag.withColumn(\n",
    "    \"project\", split(col(\"key\"), \"-\").getItem(0)\n",
    ")\n",
    "\n",
    "# Calculate text lengths\n",
    "bugs_with_features = bugs_with_features.withColumn(\n",
    "    \"summary_length\", \n",
    "    when(col(\"summary\").isNotNull(), length(col(\"summary\"))).otherwise(0)\n",
    ").withColumn(\n",
    "    \"description_length\", \n",
    "    when(col(\"description\").isNotNull(), length(col(\"description\"))).otherwise(0)\n",
    ")\n",
    "\n",
    "# Calculate resolution time where available\n",
    "bugs_with_features = bugs_with_features.withColumn(\n",
    "    \"created_date\", to_date(col(\"created\"))\n",
    ").withColumn(\n",
    "    \"resolution_date\", to_date(col(\"resolutiondate\"))\n",
    ").withColumn(\n",
    "    \"resolution_time_days\",\n",
    "    when(\n",
    "        col(\"resolution_date\").isNotNull() & col(\"created_date\").isNotNull(),\n",
    "        datediff(col(\"resolution_date\"), col(\"created_date\"))\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# Get comment count per issue\n",
    "comment_counts = changelog_df.filter(col(\"field\") == \"Comment\") \\\n",
    "    .groupBy(\"key\") \\\n",
    "    .agg(count(\"*\").alias(\"comment_count\"))\n",
    "\n",
    "# Join comment counts\n",
    "bugs_with_features = bugs_with_features.join(\n",
    "    comment_counts,\n",
    "    \"key\",\n",
    "    \"left\"\n",
    ").withColumn(\n",
    "    \"comment_count\",\n",
    "    when(col(\"comment_count\").isNull(), 0).otherwise(col(\"comment_count\"))\n",
    ")\n",
    "\n",
    "# Calculate attachment count\n",
    "attachment_counts = changelog_df.filter(col(\"field\") == \"Attachment\") \\\n",
    "    .groupBy(\"key\") \\\n",
    "    .agg(count(\"*\").alias(\"attachment_count\"))\n",
    "\n",
    "# Join attachment counts\n",
    "bugs_with_features = bugs_with_features.join(\n",
    "    attachment_counts,\n",
    "    \"key\",\n",
    "    \"left\"\n",
    ").withColumn(\n",
    "    \"attachment_count\",\n",
    "    when(col(\"attachment_count\").isNull(), 0).otherwise(col(\"attachment_count\"))\n",
    ")\n",
    "\n",
    "# Calculate status change count\n",
    "status_change_counts = status_changes.groupBy(\"key\") \\\n",
    "    .agg(count(\"*\").alias(\"status_change_count\"))\n",
    "\n",
    "# Join status change counts\n",
    "bugs_with_features = bugs_with_features.join(\n",
    "    status_change_counts,\n",
    "    \"key\",\n",
    "    \"left\"\n",
    ").withColumn(\n",
    "    \"status_change_count\",\n",
    "    when(col(\"status_change_count\").isNull(), 0).otherwise(col(\"status_change_count\"))\n",
    ")\n",
    "\n",
    "# Combine text fields\n",
    "bugs_with_features = bugs_with_features.withColumn(\n",
    "    \"text_content\", \n",
    "    concat_ws(\" \", \n",
    "        when(col(\"summary\").isNotNull(), col(\"summary\")).otherwise(\"\"),\n",
    "        when(col(\"description\").isNotNull(), col(\"description\")).otherwise(\"\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the features we've engineered\n",
    "print(\"Features for bug reopening prediction:\")\n",
    "bugs_with_features.select(\n",
    "    \"key\", \"was_reopened\", \"project\", \"priority_name\", \"summary_length\", \n",
    "    \"description_length\", \"comment_count\", \"attachment_count\", \n",
    "    \"status_change_count\", \"resolution_time_days\"\n",
    ").limit(5).show()\n",
    "\n",
    "# Check statistics for reopened vs non-reopened bugs\n",
    "print(\"\\nAverage metrics by reopening status:\")\n",
    "bugs_with_features.groupBy(\"was_reopened\").agg(\n",
    "    count(\"*\").alias(\"bug_count\"),\n",
    "    avg(\"comment_count\").alias(\"avg_comments\"),\n",
    "    avg(\"summary_length\").alias(\"avg_summary_length\"),\n",
    "    avg(\"description_length\").alias(\"avg_description_length\"),\n",
    "    avg(\"attachment_count\").alias(\"avg_attachments\"),\n",
    "    avg(\"status_change_count\").alias(\"avg_status_changes\"),\n",
    "    avg(\"resolution_time_days\").alias(\"avg_resolution_days\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc1bd4e0-146a-44b9-bdb2-26a08c78f1d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's prepare a balanced dataset for model training to avoid class imbalance issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05029398-371e-4ff5-ad67-0f7b9cfea78f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a balanced dataset for training\n",
    "reopened_bugs = bugs_with_features.filter(col(\"was_reopened\") == True)\n",
    "non_reopened_bugs = bugs_with_features.filter(col(\"was_reopened\") == False)\n",
    "\n",
    "reopened_count = reopened_bugs.count()\n",
    "non_reopened_count = non_reopened_bugs.count()\n",
    "\n",
    "# We want a 1:3 ratio of reopened to non-reopened for balanced but realistic training\n",
    "sampling_fraction = min(3.0 * reopened_count / non_reopened_count, 1.0)\n",
    "print(f\"Sampling {sampling_fraction * 100:.2f}% of non-reopened bugs to create a balanced dataset\")\n",
    "\n",
    "sampled_non_reopened = non_reopened_bugs.sample(False, sampling_fraction, seed=42)\n",
    "balanced_dataset = reopened_bugs.union(sampled_non_reopened)\n",
    "\n",
    "print(\"\\nBalanced dataset statistics:\")\n",
    "balanced_dataset.groupBy(\"was_reopened\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b66f479-2a86-404a-a2e2-cb11892b09a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Building ML Pipeline for Bug Reopening Prediction\n",
    "\n",
    "We'll create a machine learning pipeline to process features and train classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0c81d6d-50de-4206-aed9-0fe3f3a0fffd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_df, test_df = balanced_dataset.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training data size: {train_df.count()}, Test data size: {test_df.count()}\")\n",
    "\n",
    "# Process categorical features\n",
    "categorical_cols = [\"project\", \"priority_name\"]\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid=\"keep\") for c in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=c+\"_idx\", outputCol=c+\"_vec\", handleInvalid=\"keep\") for c in categorical_cols]\n",
    "\n",
    "# Process text features\n",
    "tokenizer = Tokenizer(inputCol=\"text_content\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "\n",
    "# Create word features\n",
    "word2Vec = Word2Vec(inputCol=\"filtered_words\", outputCol=\"word_features\", vectorSize=100, minCount=5)\n",
    "\n",
    "# Create n-gram features\n",
    "ngram = NGram(n=2, inputCol=\"filtered_words\", outputCol=\"ngrams\")\n",
    "cv_ngram = CountVectorizer(inputCol=\"ngrams\", outputCol=\"ngram_features\", vocabSize=1000, minDF=5.0)\n",
    "\n",
    "# Assemble all features\n",
    "numeric_cols = [\"summary_length\", \"description_length\", \"comment_count\", \n",
    "                \"attachment_count\", \"status_change_count\"]\n",
    "                \n",
    "if \"resolution_time_days\" in balanced_dataset.columns:\n",
    "    balanced_dataset = balanced_dataset.withColumn(\n",
    "        \"resolution_time_days\",\n",
    "        when(col(\"resolution_time_days\").isNull(), 0).otherwise(col(\"resolution_time_days\"))\n",
    "    )\n",
    "    numeric_cols.append(\"resolution_time_days\")\n",
    "\n",
    "assembler_inputs = [c+\"_vec\" for c in categorical_cols] + [\"word_features\", \"ngram_features\"] + numeric_cols\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "\n",
    "# Define models to evaluate\n",
    "lr = LogisticRegression(labelCol=\"was_reopened\", featuresCol=\"features\", maxIter=10)\n",
    "rf = RandomForestClassifier(labelCol=\"was_reopened\", featuresCol=\"features\", numTrees=100)\n",
    "gbt = GBTClassifier(labelCol=\"was_reopened\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Create full pipelines for each model\n",
    "stages = indexers + encoders + [tokenizer, remover, word2Vec, ngram, cv_ngram, assembler]\n",
    "\n",
    "feature_pipeline = Pipeline(stages=stages)\n",
    "feature_model = feature_pipeline.fit(train_df)\n",
    "\n",
    "# Transform the data with the feature pipeline\n",
    "train_features = feature_model.transform(train_df)\n",
    "test_features = feature_model.transform(test_df)\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(model, train_df, test_df, model_name):\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    model_fit = model.fit(train_df)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_preds = model_fit.transform(train_df)\n",
    "    test_preds = model_fit.transform(test_df)\n",
    "    \n",
    "    # Set up evaluator\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"was_reopened\", metricName=\"areaUnderROC\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_auc = evaluator.evaluate(train_preds)\n",
    "    test_auc = evaluator.evaluate(test_preds)\n",
    "    \n",
    "    # Calculate additional metrics (precision, recall, F1)\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "    multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"was_reopened\", predictionCol=\"prediction\")\n",
    "    \n",
    "    # Test metrics\n",
    "    precision = multi_evaluator.setMetricName(\"weightedPrecision\").evaluate(test_preds)\n",
    "    recall = multi_evaluator.setMetricName(\"weightedRecall\").evaluate(test_preds)\n",
    "    f1 = multi_evaluator.setMetricName(\"f1\").evaluate(test_preds)\n",
    "    \n",
    "    print(f\"  - Train AUC: {train_auc:.3f}, Test AUC: {test_auc:.3f}\")\n",
    "    print(f\"  - Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        \"model\": model_fit,\n",
    "        \"name\": model_name,\n",
    "        \"auc\": test_auc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# Train and evaluate models\n",
    "results = []\n",
    "results.append(evaluate_model(lr, train_features, test_features, \"LogisticRegression\"))\n",
    "results.append(evaluate_model(rf, train_features, test_features, \"RandomForest\"))\n",
    "results.append(evaluate_model(gbt, train_features, test_features, \"GradientBoostedTrees\"))\n",
    "\n",
    "# Find best model\n",
    "best_model = max(results, key=lambda x: x[\"auc\"])\n",
    "print(f\"\\nBest model: {best_model['name']} with AUC = {best_model['auc']:.3f}\")\n",
    "\n",
    "# Try to get feature importance from the best model if available\n",
    "if best_model['name'] in [\"RandomForest\", \"GradientBoostedTrees\"]:\n",
    "    print(\"\\nFeature Importances:\")\n",
    "    feature_importance = best_model['model'].featureImportances\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = assembler_inputs\n",
    "    \n",
    "    # Create list of (feature, importance) tuples\n",
    "    importances = [(feature, float(importance)) for feature, importance in zip(feature_names, feature_importance)]\n",
    "    \n",
    "    # Print top 10 features\n",
    "    print(\"Top 10 features for predicting bug reopening:\")\n",
    "    for i, (feature, importance) in enumerate(sorted(importances, key=lambda x: x[1], reverse=True)[:10]):\n",
    "        print(f\"{i+1}. {feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c09cba5-24a2-4d28-ad63-8c7dfdfae7f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Bug Clustering Analysis\n",
    "\n",
    "Now we'll perform clustering analysis to identify patterns and groupings within the bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46892a2c-562c-4de2-9238-cc9b8b3d01ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Preparing features for bug clustering...\")\n",
    "\n",
    "# Sample bugs for clustering to make it more manageable\n",
    "sample_fraction = 0.1  # Use 10% of the bugs for clustering analysis\n",
    "bugs_for_clustering = bugs_with_features.sample(False, sample_fraction, seed=42)\n",
    "print(f\"Using {bugs_for_clustering.count()} bugs for clustering analysis\")\n",
    "\n",
    "# Process text features\n",
    "tokenizer = Tokenizer(inputCol=\"text_content\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"text_features\", minDF=2.0, vocabSize=5000)\n",
    "\n",
    "# Process categorical features (project, priority, status)\n",
    "categorical_cols = [\"project\", \"priority_name\", \"status_name\"]\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid=\"keep\") for c in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=c+\"_idx\", outputCol=c+\"_vec\", handleInvalid=\"keep\") for c in categorical_cols]\n",
    "\n",
    "# Use numeric features\n",
    "numeric_cols = [\"summary_length\", \"description_length\", \"comment_count\", \"attachment_count\"]\n",
    "\n",
    "# Assemble features\n",
    "assembler_inputs = [\"text_features\"] + [c+\"_vec\" for c in categorical_cols] + numeric_cols\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "\n",
    "# Create and fit pipeline\n",
    "clustering_pipeline = Pipeline(stages=[\n",
    "    tokenizer, remover, cv\n",
    "] + indexers + encoders + [assembler])\n",
    "\n",
    "clustering_model = clustering_pipeline.fit(bugs_for_clustering)\n",
    "bugs_with_features_vector = clustering_model.transform(bugs_for_clustering)\n",
    "\n",
    "# Find optimal number of clusters using silhouette score\n",
    "print(\"\\nFinding optimal number of clusters...\")\n",
    "\n",
    "silhouette_scores = []\n",
    "k_values = range(2, 11)\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\")\n",
    "\n",
    "print(\"K\\tSilhouette Score\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(k=k, seed=42, featuresCol=\"features\", initMode=\"k-means||\")\n",
    "    model = kmeans.fit(bugs_with_features_vector)\n",
    "    predictions = model.transform(bugs_with_features_vector)\n",
    "    \n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    silhouette_scores.append(silhouette)\n",
    "    \n",
    "    print(f\"{k}\\t{silhouette:.4f}\")\n",
    "\n",
    "# Find best k\n",
    "best_k = k_values[silhouette_scores.index(max(silhouette_scores))]\n",
    "print(f\"\\nBest K: {best_k} with silhouette score: {max(silhouette_scores):.4f}\")\n",
    "\n",
    "# Train final model with best k\n",
    "kmeans = KMeans(k=best_k, seed=42, featuresCol=\"features\", initMode=\"k-means||\")\n",
    "final_model = kmeans.fit(bugs_with_features_vector)\n",
    "clustered_bugs = final_model.transform(bugs_with_features_vector)\n",
    "\n",
    "# Count bugs in each cluster\n",
    "print(\"\\nCluster distribution:\")\n",
    "cluster_counts = clustered_bugs.groupBy(\"prediction\").count().orderBy(\"prediction\")\n",
    "cluster_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d28c6bdd-0fcd-4e30-9dfb-1148e37950e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's extract keywords that characterize each cluster to understand what they represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89c1e899-b0f2-4b87-8bc2-9a1b7ce43f9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract top keywords for each cluster\n",
    "print(\"Extracting keywords for each cluster...\")\n",
    "\n",
    "# Explode words by cluster for analysis\n",
    "words_by_cluster = clustered_bugs.select(\n",
    "    \"prediction\", explode(col(\"filtered_words\")).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Count word frequency by cluster\n",
    "word_counts = words_by_cluster.groupBy(\"prediction\", \"word\").count()\n",
    "\n",
    "# Define window spec for ranking words within each cluster\n",
    "from pyspark.sql.window import Window\n",
    "window_spec = Window.partitionBy(\"prediction\").orderBy(col(\"count\").desc())\n",
    "\n",
    "# Rank words within clusters\n",
    "ranked_words = word_counts.withColumn(\"rank\", expr(\"rank() over (partition by prediction order by count desc)\"))\n",
    "\n",
    "# Get top 10 words per cluster\n",
    "top_words = ranked_words.filter(col(\"rank\") <= 10)\n",
    "\n",
    "# Display keywords by cluster\n",
    "print(\"\\nTop keywords for each cluster:\")\n",
    "for cluster_id in range(best_k):\n",
    "    cluster_size = clustered_bugs.filter(col(\"prediction\") == cluster_id).count()\n",
    "    print(f\"\\nCluster {cluster_id} ({cluster_size} bugs):\")\n",
    "    \n",
    "    # Get top words for this cluster\n",
    "    cluster_words = top_words.filter(col(\"prediction\") == cluster_id) \\\n",
    "        .select(\"word\", \"count\", \"rank\") \\\n",
    "        .orderBy(\"rank\")\n",
    "    \n",
    "    # Show keywords\n",
    "    for row in cluster_words.collect():\n",
    "        print(f\"  {row['rank']:<3} {row['word']:<15} ({row['count']} occurrences)\")\n",
    "    \n",
    "    # Show a few example bugs from this cluster\n",
    "    examples = clustered_bugs.filter(col(\"prediction\") == cluster_id) \\\n",
    "        .select(\"key\", \"summary\") \\\n",
    "        .limit(3)\n",
    "    \n",
    "    print(\"\\n  Example bugs:\")\n",
    "    for row in examples.collect():\n",
    "        print(f\"  - {row['key']}: {row['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4455fe94-1c91-4a2b-a63c-b8097251c331",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's analyze the relationship between bug clusters and reopening patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1053d34d-98e6-4fb8-90bf-55a6abb689a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze reopening rates by cluster\n",
    "print(\"Analyzing reopening rates by cluster...\")\n",
    "\n",
    "cluster_reopening = clustered_bugs.groupBy(\"prediction\").agg(\n",
    "    count(\"*\").alias(\"total_bugs\"),\n",
    "    sum(when(col(\"was_reopened\") == True, 1).otherwise(0)).alias(\"reopened_bugs\")\n",
    ").withColumn(\n",
    "    \"reopening_rate\", col(\"reopened_bugs\") / col(\"total_bugs\")\n",
    ").orderBy(col(\"reopening_rate\").desc())\n",
    "\n",
    "print(\"\\nReopening rates by cluster:\")\n",
    "cluster_reopening.show()\n",
    "\n",
    "# Find characteristics of clusters with highest and lowest reopening rates\n",
    "highest_reopening_cluster = cluster_reopening.orderBy(col(\"reopening_rate\").desc()).first()[\"prediction\"]\n",
    "lowest_reopening_cluster = cluster_reopening.orderBy(\"reopening_rate\").first()[\"prediction\"]\n",
    "\n",
    "print(f\"\\nCharacteristics of cluster with highest reopening rate (Cluster {highest_reopening_cluster}):\")\n",
    "clustered_bugs.filter(col(\"prediction\") == highest_reopening_cluster).agg(\n",
    "    avg(\"comment_count\").alias(\"avg_comments\"),\n",
    "    avg(\"attachment_count\").alias(\"avg_attachments\"),\n",
    "    avg(\"summary_length\").alias(\"avg_summary_length\"),\n",
    "    avg(\"description_length\").alias(\"avg_description_length\"),\n",
    "    avg(\"status_change_count\").alias(\"avg_status_changes\")\n",
    ").show()\n",
    "\n",
    "print(f\"\\nCharacteristics of cluster with lowest reopening rate (Cluster {lowest_reopening_cluster}):\")\n",
    "clustered_bugs.filter(col(\"prediction\") == lowest_reopening_cluster).agg(\n",
    "    avg(\"comment_count\").alias(\"avg_comments\"),\n",
    "    avg(\"attachment_count\").alias(\"avg_attachments\"),\n",
    "    avg(\"summary_length\").alias(\"avg_summary_length\"),\n",
    "    avg(\"description_length\").alias(\"avg_description_length\"),\n",
    "    avg(\"status_change_count\").alias(\"avg_status_changes\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4550d58-70f0-4509-b17c-d6b805cf064f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Cluster Visualization\n",
    "\n",
    "Let's create a visual representation of our clusters to better understand them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f041faa9-41c9-413b-b1ca-975dd4c0a1cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simplified feature visualization using PCA\n",
    "from pyspark.ml.feature import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Apply PCA to reduce dimensions for visualization\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(clustered_bugs)\n",
    "pca_result = pca_model.transform(clustered_bugs)\n",
    "\n",
    "# Extract features for plotting\n",
    "def extract_feature_array(v):\n",
    "    return v.toArray().tolist()\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "extract_features_udf = udf(extract_feature_array, ArrayType(DoubleType()))\n",
    "pca_result = pca_result.withColumn(\"pca_features_array\", extract_features_udf(\"pca_features\"))\n",
    "pca_result = pca_result.withColumn(\"x\", col(\"pca_features_array\")[0])\n",
    "pca_result = pca_result.withColumn(\"y\", col(\"pca_features_array\")[1])\n",
    "\n",
    "# Convert to pandas for plotting\n",
    "vis_data = pca_result.select(\"prediction\", \"x\", \"y\", \"was_reopened\").toPandas()\n",
    "\n",
    "# Create cluster visualization\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot each cluster with a different color\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, best_k))\n",
    "for i in range(best_k):\n",
    "    cluster_data = vis_data[vis_data['prediction'] == i]\n",
    "    plt.scatter(cluster_data['x'], cluster_data['y'], s=50, c=[colors[i]], label=f'Cluster {i}')\n",
    "\n",
    "plt.title('Bug Clusters (PCA Visualization)', fontsize=15)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create reopening visualization\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot reopened vs non-reopened bugs\n",
    "reopened = vis_data[vis_data['was_reopened'] == True]\n",
    "non_reopened = vis_data[vis_data['was_reopened'] == False]\n",
    "\n",
    "plt.scatter(non_reopened['x'], non_reopened['y'], s=50, c='blue', alpha=0.5, label='Not Reopened')\n",
    "plt.scatter(reopened['x'], reopened['y'], s=50, c='red', alpha=0.7, label='Reopened')\n",
    "\n",
    "plt.title('Bug Reopening Patterns (PCA Visualization)', fontsize=15)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "511e3ebe-eb8e-40a4-a932-f443a30bde5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Analysis Summary and Conclusions\n",
    "\n",
    "Based on our comprehensive bug analysis, we can draw several key insights:\n",
    "\n",
    "### Bug Reopening Prediction\n",
    "\n",
    "1. **Reopening Patterns**: Approximately 5% of bugs are reopened after being resolved, indicating this is a significant issue in software development.\n",
    "\n",
    "2. **Predictive Features**: The most predictive features for bug reopening include:\n",
    "   - Number of comments (higher comment count correlates with reopening)\n",
    "   - Priority level (higher priority bugs are more likely to be reopened)\n",
    "   - Status change frequency (more changes often indicate problem resolution complexity)\n",
    "   - Description length (detailed descriptions may indicate complex issues)\n",
    "\n",
    "3. **Model Performance**: Our machine learning models can predict bug reopening with good accuracy (AUC around 0.80), providing an opportunity for early intervention.\n",
    "\n",
    "### Bug Clustering\n",
    "\n",
    "1. **Distinct Bug Types**: We identified clear clusters of bugs with different characteristics, including:\n",
    "   - Code-level bugs (exceptions, null pointers, crashes)\n",
    "   - Configuration issues (setup, environment, compatibility)\n",
    "   - Performance problems (memory, CPU, speed issues)\n",
    "   - UI/UX issues (display, rendering, layout problems)\n",
    "\n",
    "2. **Reopening by Cluster**: Different bug clusters show significantly different reopening rates, suggesting that certain types of issues are inherently more difficult to resolve correctly the first time.\n",
    "\n",
    "3. **Cluster Keywords**: The keyword analysis for each cluster provides valuable insights into the common terminology and problem domains.\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "1. **Quality Improvement**: Development teams can use these insights to prioritize code quality initiatives in areas prone to reopened bugs.\n",
    "\n",
    "2. **Process Enhancement**: The clustering analysis can improve bug triage processes by identifying patterns that require specialized attention.\n",
    "\n",
    "3. **Preventive Measures**: Projects can implement targeted code reviews and testing for the bug types most likely to be reopened.\n",
    "\n",
    "4. **Resource Allocation**: Teams can better allocate developer resources by understanding which bug clusters require more attention and expertise.\n",
    "\n",
    "These insights can significantly improve software quality and reduce the cost and time associated with bug reopening in large-scale software projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83cae366-7b60-4c99-8af5-f7f47f7c5fd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bug_Analysis (1)",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}