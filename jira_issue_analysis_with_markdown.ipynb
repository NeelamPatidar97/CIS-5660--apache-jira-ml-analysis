{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "425025c7",
      "metadata": {
        "id": "425025c7"
      },
      "source": [
        "# ðŸ“Š Analyzing Software Issue Tracking and Resolution Trends: Insights from Apache JIRA\n",
        "\n",
        "This project analyzes issue tracking, resolution patterns, and changelog activity using a public Apache JIRA dataset. Leveraging PySpark, we examine issue lifecycles, team collaboration, and resolution times. The analysis includes engineered features, grouping strategies, and filtering based on time (last two years) to generate valuable insights.\n",
        "\n",
        "---\n",
        "## ðŸ§¹ Data Preprocessing\n",
        "The dataset consists of three main components:\n",
        "- `issues.csv`: metadata about each JIRA issue.\n",
        "- `changelog.csv`: tracks status changes and field updates.\n",
        "- `comments.csv`: discussion history tied to each issue.\n",
        "\n",
        "We performed the following preprocessing steps:\n",
        "- Cleaned column names by removing dots/spaces.\n",
        "- Dropped duplicates based on unique identifiers (`key`, `comment_id`, `id`).\n",
        "- Removed rows with nulls in required columns.\n",
        "- Filtered malformed timestamps (e.g., those not starting with '20').\n",
        "\n",
        "---\n",
        "## ðŸ§® Feature Engineering\n",
        "We enriched the data with:\n",
        "- `comment_count`: Number of comments per issue.\n",
        "- `status_change_count`: Number of status field changes per issue.\n",
        "- `resolution_days`: Time taken (in days) to resolve an issue.\n",
        "\n",
        "Only issues created in the last **2 years** and with valid `resolution_days` were retained for analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f04813eb",
      "metadata": {
        "id": "f04813eb"
      },
      "source": [
        "## ðŸ“ˆ Q1: What is the distribution of issue resolution times for the last two years?\n",
        "\n",
        "By filtering issues created in the last 24 months and calculating `resolution_days`, we isolate a subset of recent issues for more relevant analysis. This allows us to:\n",
        "- Avoid outdated project workflows influencing results.\n",
        "- Focus on modern triage patterns and prioritization practices.\n",
        "\n",
        "This cleaned and scoped dataset is exported for further visualization.\n",
        "\n",
        "---\n",
        "## ðŸ“Š Q2: How does resolution time vary across issue priorities and statuses?\n",
        "\n",
        "We grouped the dataset by `priority_name` and `status_name`, calculating the average resolution time (`avg_resolution_days`) for each combination.\n",
        "\n",
        "### ðŸ” Interpretation:\n",
        "- High-priority issues (e.g., Blocker, Critical) are generally expected to resolve faster.\n",
        "- However, variations within statuses (like â€œIn Progressâ€ or â€œReviewâ€) reveal inefficiencies or bottlenecks.\n",
        "- Average resolution times can identify whether priorities are enforced correctly or overlooked during execution.\n",
        "\n",
        "This result is exported for visualization via Tableau to generate comparative stacked bars.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1f23846",
      "metadata": {
        "id": "f1f23846"
      },
      "source": [
        "## ðŸ”„ Q3: What are the top 10 most frequently changed fields in JIRA issues?\n",
        "\n",
        "We analyzed the `changelog.csv` to determine:\n",
        "- Fields with the **most total changes**.\n",
        "- How many **unique issues** were affected per field.\n",
        "\n",
        "### ðŸ“Œ Insights:\n",
        "- High change frequency in fields like `status`, `priority`, or `assignee` can indicate process fluidity or inconsistency.\n",
        "- This helps identify areas where workflow policies may need to be refined or automated.\n",
        "\n",
        "The results were exported for additional reporting and graph generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7964a8e",
      "metadata": {
        "id": "d7964a8e"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import unix_timestamp, col, round, current_timestamp, expr, to_timestamp\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"ApacheJiraML\").getOrCreate()\n",
        "\n",
        "# Fix for Spark 3+ timestamp parsing issues\n",
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
        "\n",
        "# Function to clean column names\n",
        "def clean_column_names(df: DataFrame) -> DataFrame:\n",
        "    for col_name in df.columns:\n",
        "        clean_name = col_name.replace(\".\", \"_\").replace(\" \", \"_\")\n",
        "        if clean_name != col_name:\n",
        "            df = df.withColumnRenamed(col_name, clean_name)\n",
        "    return df\n",
        "\n",
        "# Load CSVs from HDFS\n",
        "issues_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).option(\"multiLine\", True).option(\"escape\", \"\\\"\").csv(\"/user/your_username/Apache_JIRA_Issues/issues.csv\")\n",
        "changelog_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).option(\"multiLine\", True).option(\"escape\", \"\\\"\").csv(\"/user/your_username/Apache_JIRA_Issues/changelog.csv\")\n",
        "comments_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).option(\"multiLine\", True).option(\"escape\", \"\\\"\").csv(\"/user/your_username/Apache_JIRA_Issues/comments.csv\")\n",
        "\n",
        "# Clean column names\n",
        "issues_df = clean_column_names(issues_df)\n",
        "changelog_df = clean_column_names(changelog_df)\n",
        "comments_df = clean_column_names(comments_df)\n",
        "\n",
        "# Deduplicate\n",
        "issues_df = issues_df.dropDuplicates([\"key\"])\n",
        "comments_df = comments_df.dropDuplicates([\"comment_id\"])\n",
        "changelog_df = changelog_df.dropDuplicates([\"id\"])\n",
        "\n",
        "# Drop rows with nulls in required columns\n",
        "issues_df = issues_df.dropna(subset=[\"key\", \"created\", \"updated\", \"status_name\", \"priority_name\", \"issuetype_name\"])\n",
        "comments_df = comments_df.dropna(subset=[\"key\", \"comment_id\"])\n",
        "changelog_df = changelog_df.dropna(subset=[\"key\", \"field\", \"id\"])\n",
        "\n",
        "# Join comment count and status change count\n",
        "comment_counts = comments_df.groupBy(\"key\").agg(F.count(\"comment_id\").alias(\"comment_count\"))\n",
        "status_change_counts = changelog_df.filter(col(\"field\") == \"status\").groupBy(\"key\").agg(F.count(\"id\").alias(\"status_change_count\"))\n",
        "\n",
        "issues_df = issues_df.join(comment_counts, \"key\", \"left\").join(status_change_counts, \"key\", \"left\") \\\n",
        "    .fillna({\"comment_count\": 0, \"status_change_count\": 0})\n",
        "\n",
        "# ðŸ” Filter out malformed timestamps (e.g., starting with 2-digit year like '12-09-17')\n",
        "issues_df = issues_df.filter(col(\"created\").startswith(\"20\")).filter(col(\"updated\").startswith(\"20\"))\n",
        "\n",
        "# Convert to timestamp\n",
        "issues_df = issues_df \\\n",
        "    .withColumn(\"created\", to_timestamp(\"created\", \"yyyy-MM-dd HH:mm:ss\")) \\\n",
        "    .withColumn(\"updated\", to_timestamp(\"updated\", \"yyyy-MM-dd HH:mm:ss\"))\n",
        "\n",
        "\n",
        "#What is the distribution of issue resolution times for last two years?\n",
        "# Calculate resolution_days\n",
        "issues_df = issues_df.withColumn(\n",
        "    \"resolution_days\",\n",
        "    round((unix_timestamp(\"updated\") - unix_timestamp(\"created\")) / 86400, 2)\n",
        ")\n",
        "\n",
        "# Filter: resolved issues in last 2 years\n",
        "two_years_ago = expr(\"add_months(current_timestamp(), -24)\")\n",
        "resolved_df = issues_df.filter(\n",
        "    (col(\"resolution_days\").isNotNull()) & (col(\"created\") >= two_years_ago)\n",
        ")\n",
        "\n",
        "# Preview\n",
        "resolved_df.select(\n",
        "    \"key\", \"created\", \"updated\", \"resolution_days\",\n",
        "    \"project_name\", \"status_name\", \"priority_name\", \"issuetype_name\"\n",
        ").show(5)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# âœ… Export to HDFS\n",
        "# Select columns for export\n",
        "export_df = resolved_df.select(\n",
        "    \"key\",\n",
        "    \"created\",\n",
        "    \"updated\",\n",
        "    \"resolution_days\",\n",
        "    \"project_name\",\n",
        "    \"status_name\",\n",
        "    \"priority_name\",\n",
        "    \"issuetype_name\",\n",
        "    \"comment_count\",\n",
        "    \"status_change_count\"\n",
        ")\n",
        "\n",
        "\n",
        "# Export as single CSV with header for visualization\n",
        "export_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"/user/npatida/output/resolved_df_q1\")\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Question 2 How does resolution time vary across issue priorities and statuses?\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Group by priority and status to get average resolution time\n",
        "priority_status_avg_df = resolved_df.groupBy(\"priority_name\", \"status_name\") \\\n",
        "    .agg(avg(\"resolution_days\").alias(\"avg_resolution_days\")) \\\n",
        "    .orderBy(\"priority_name\", \"status_name\")\n",
        "\n",
        "priority_status_avg_df.show(5)\n",
        "\n",
        "\"\"\"\n",
        "# Save to HDFS for Tableau or Excel\n",
        "priority_status_avg_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True) \\\n",
        "    .csv(\"/user/npatida/output/q2_priority_status_resolution\")\n",
        "\"\"\"\n",
        "\n",
        "# Q3: Top 10 fields changed most frequently + how many unique issues were affected\n",
        "changelog_summary_df = changelog_df.groupBy(\"field\") \\\n",
        "    .agg(\n",
        "        F.count(\"*\").alias(\"total_changes\"),\n",
        "        F.countDistinct(\"key\").alias(\"affected_issues\")\n",
        "    ) \\\n",
        "    .orderBy(F.desc(\"total_changes\")) \\\n",
        "    .limit(10)\n",
        "\n",
        "# Show the summary\n",
        "changelog_summary_df.show(truncate=False)\n",
        "\n",
        "# Save to HDFS for reporting\n",
        "changelog_summary_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True) \\\n",
        "    .csv(\"/user/your_username/output/q3_field_change_summary\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}