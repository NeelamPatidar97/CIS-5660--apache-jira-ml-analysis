{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8db3526c-13d0-4ce6-961f-ca94543264dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìù Apache JIRA Comment Sentiment Analysis\n",
    "\n",
    "## Overview\n",
    "This notebook performs sentiment analysis on JIRA issue comments, evaluates classification models, extracts discussion topics, and analyzes correlations with issue resolution times, incorporating:\n",
    "- Text cleaning and preprocessing\n",
    "- Sentiment classification (rule-based and logistic regression)\n",
    "- Model evaluation metrics\n",
    "- Topic extraction with LDA\n",
    "- Correlation analysis with issue metrics\n",
    "\n",
    "Each section is documented with explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b084c69-990a-4351-91ea-dd3b4e4d18d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üèóÔ∏è Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b895e59-4eed-4a4d-9e84-b4249506a20c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, count, length, lower, regexp_replace, udf, \n",
    "    collect_list, explode, array_contains, lit, avg, max, min,\n",
    "    concat_ws, split, trim, size\n",
    ")\n",
    "from pyspark.sql.types import FloatType, ArrayType, StringType, IntegerType\n",
    "from pyspark.ml.feature import (\n",
    "    Tokenizer, StopWordsRemover, CountVectorizer, IDF,\n",
    "    StringIndexer, VectorAssembler \n",
    ")\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa2d7516-c625-4166-9c14-8560ec420dab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üöÄ Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dac5c15-59ca-4567-ba1e-37c5e9029460",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Initializing Spark session...\")\n",
    "spark = SparkSession.builder     .appName(\"Enhanced Comment Sentiment Analysis\")     .config(\"spark.driver.memory\", \"2g\")     .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "694b9e41-98af-4a84-aab4-258ba2399ee2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìÇ Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fd9794b-cfb8-473e-9c54-2e86a496b9fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "comments_path = \"/user/szreiqa/Apache_JIRA_Issues/cleaned_comments.parquet\"\n",
    "issues_path = \"/user/szreiqa/Apache_JIRA_Issues/cleaned_issues.parquet\"\n",
    "comments_df = spark.read.parquet(comments_path)\n",
    "print(f\"Total comments: {comments_df.count()}\")\n",
    "try:\n",
    "    issues_df = spark.read.parquet(issues_path)\n",
    "    print(f\"Total issues: {issues_df.count()}\")\n",
    "    has_issues = True\n",
    "except:\n",
    "    print(\"Issues data not available or could not be loaded\")\n",
    "    has_issues = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36a46afc-3759-4bfc-a60b-0192d4793cb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîç Identify Comment Body Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17a6909c-690e-4568-ae84-8cfec58950a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "body_column = None\n",
    "for column in [\"comment_body\", \"body\", \"comment_text\", \"text\"]:\n",
    "    if column in comments_df.columns:\n",
    "        body_column = column\n",
    "        break\n",
    "if not body_column:\n",
    "    print(\"Could not find comment body column\")\n",
    "    exit(1)\n",
    "print(f\"Using column: {body_column}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb766c09-cb2c-486e-ad01-c13c6802ad9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üßπ Text Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca92d03a-4248-46dd-b2c8-dba3a1734d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_comments = comments_df.withColumn(\n",
    "    \"cleaned_text\", \n",
    "    regexp_replace(\n",
    "        regexp_replace(\n",
    "            regexp_replace(lower(col(body_column)), r\"<[^>]+>\", \"\"),\n",
    "            r\"http\\S+\", \"\"\n",
    "        ),\n",
    "        r\"[^a-zA-Z\\s]\", \" \"\n",
    "    )\n",
    ")\n",
    "filtered_comments = cleaned_comments.filter(length(col(\"cleaned_text\")) > 10)\n",
    "print(f\"Comments after filtering: {filtered_comments.count()}\")\n",
    "if filtered_comments.count() > 50000:\n",
    "    print(\"Sampling comments for faster processing...\")\n",
    "    filtered_comments = filtered_comments.sample(False, 50000.0/filtered_comments.count(), seed=42)\n",
    "    print(f\"Sampled to {filtered_comments.count()} comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e384a716-a60a-4eeb-8861-56f908980de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üßÆ Sentiment Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbccd0d5-94c3-463e-b43c-7b73a5edc4b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Fixed Comment Sentiment Analysis with Topics\n",
    "\n",
    "This script analyzes sentiment in JIRA comments with fixed topic modeling\n",
    "implementation to avoid the error with explode function on DenseVector.\n",
    "\n",
    "Use: spark-submit fixed_sentiment_analysis.py\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, count, length, lower, regexp_replace, udf, \n",
    "    collect_list, lit, avg, max, min, array, explode, struct\n",
    ")\n",
    "from pyspark.sql.types import FloatType, ArrayType, StringType, IntegerType\n",
    "from pyspark.ml.feature import (\n",
    "    Tokenizer, StopWordsRemover, CountVectorizer, IDF,\n",
    "    StringIndexer, VectorAssembler \n",
    ")\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Initialize Spark session\n",
    "print(\"Initializing Spark session...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Fixed Comment Sentiment Analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"Loading data...\")\n",
    "comments_path = \"/user/szreiqa/Apache_JIRA_Issues/cleaned_comments.parquet\"\n",
    "issues_path = \"/user/szreiqa/Apache_JIRA_Issues/cleaned_issues.parquet\"\n",
    "\n",
    "comments_df = spark.read.parquet(comments_path)\n",
    "print(f\"Total comments: {comments_df.count()}\")\n",
    "\n",
    "# Try to load issues for additional analysis\n",
    "try:\n",
    "    issues_df = spark.read.parquet(issues_path)\n",
    "    print(f\"Total issues: {issues_df.count()}\")\n",
    "    has_issues = True\n",
    "except:\n",
    "    print(\"Issues data not available or could not be loaded\")\n",
    "    has_issues = False\n",
    "\n",
    "# Find comment body column\n",
    "body_column = None\n",
    "for column in [\"comment_body\", \"body\", \"comment_text\", \"text\"]:\n",
    "    if column in comments_df.columns:\n",
    "        body_column = column\n",
    "        break\n",
    "\n",
    "if not body_column:\n",
    "    print(\"Could not find comment body column\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"Using column: {body_column}\")\n",
    "\n",
    "# Clean and prepare text\n",
    "print(\"Cleaning and preparing text...\")\n",
    "cleaned_comments = comments_df.withColumn(\n",
    "    \"cleaned_text\", \n",
    "    regexp_replace(\n",
    "        regexp_replace(\n",
    "            regexp_replace(lower(col(body_column)), \n",
    "                r\"<[^>]+>\", \"\"  # Remove HTML\n",
    "            ), \n",
    "            r\"http\\S+\", \"\"  # Remove URLs\n",
    "        ),\n",
    "        r\"[^a-zA-Z\\s]\", \" \"  # Keep only letters and spaces\n",
    "    )\n",
    ")\n",
    "\n",
    "# Filter out very short comments\n",
    "filtered_comments = cleaned_comments.filter(length(col(\"cleaned_text\")) > 10)\n",
    "print(f\"Comments after filtering: {filtered_comments.count()}\")\n",
    "\n",
    "# Sample if too many comments to process\n",
    "if filtered_comments.count() > 10000:\n",
    "    print(\"Sampling comments for faster processing...\")\n",
    "    filtered_comments = filtered_comments.sample(False, 10000.0/filtered_comments.count(), seed=42)\n",
    "    print(f\"Sampled to {filtered_comments.count()} comments\")\n",
    "\n",
    "# Define sentiment patterns\n",
    "positive_patterns = [\n",
    "    \"thank\", \"good\", \"great\", \"excellent\", \"awesome\",\n",
    "    \"works\", \"working\", \"fixed\", \"resolved\", \"solved\",\n",
    "    \"correct\", \"perfect\", \"nice\", \"love\", \"happy\",\n",
    "    \"helpful\", \"appreciated\", \"appreciate\"\n",
    "]\n",
    "\n",
    "negative_patterns = [\n",
    "    \"bug\", \"issue\", \"problem\", \"error\", \"fail\",\n",
    "    \"failed\", \"wrong\", \"bad\", \"broken\", \"not work\",\n",
    "    \"doesn't work\", \"incorrect\", \"crash\", \"stuck\",\n",
    "    \"terrible\", \"horrible\", \"awful\", \"disappointing\"\n",
    "]\n",
    "\n",
    "# Rule-based sentiment scoring function\n",
    "def calculate_sentiment(text):\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    \n",
    "    text = text.lower()\n",
    "    pos_count = sum(1 for pattern in positive_patterns if pattern in text)\n",
    "    neg_count = sum(1 for pattern in negative_patterns if pattern in text)\n",
    "    \n",
    "    if pos_count == 0 and neg_count == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate normalized score\n",
    "    return (pos_count - neg_count) / (pos_count + neg_count)\n",
    "\n",
    "# Register UDF for sentiment calculation\n",
    "sentiment_udf = udf(calculate_sentiment, FloatType())\n",
    "\n",
    "# Calculate sentiment scores\n",
    "print(\"Calculating sentiment scores...\")\n",
    "comments_with_sentiment = filtered_comments.withColumn(\n",
    "    \"sentiment_score\", sentiment_udf(col(\"cleaned_text\"))\n",
    ")\n",
    "\n",
    "# Categorize sentiment\n",
    "comments_with_sentiment = comments_with_sentiment.withColumn(\n",
    "    \"sentiment_category\",\n",
    "    when(col(\"sentiment_score\") <= -0.6, \"Very Negative\")\n",
    "    .when(col(\"sentiment_score\") <= -0.2, \"Negative\")\n",
    "    .when(col(\"sentiment_score\") <= 0.2, \"Neutral\")\n",
    "    .when(col(\"sentiment_score\") <= 0.6, \"Positive\")\n",
    "    .otherwise(\"Very Positive\")\n",
    ")\n",
    "\n",
    "# Display sentiment distribution\n",
    "print(\"\\nComment sentiment distribution:\")\n",
    "sentiment_distribution = comments_with_sentiment.groupBy(\"sentiment_category\").count().orderBy(\n",
    "    when(col(\"sentiment_category\") == \"Very Negative\", 1)\n",
    "    .when(col(\"sentiment_category\") == \"Negative\", 2)\n",
    "    .when(col(\"sentiment_category\") == \"Neutral\", 3)\n",
    "    .when(col(\"sentiment_category\") == \"Positive\", 4)\n",
    "    .when(col(\"sentiment_category\") == \"Very Positive\", 5)\n",
    ")\n",
    "sentiment_distribution.show()\n",
    "\n",
    "# Prepare data for ML evaluation similar to lab example\n",
    "print(\"\\nPreparing data for ML evaluation...\")\n",
    "\n",
    "# StringIndexer for sentiment labels\n",
    "indexer = StringIndexer(\n",
    "    inputCol=\"sentiment_category\", \n",
    "    outputCol=\"label\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# ML pipeline for text features\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"features\", minDF=2.0, vocabSize=10000)\n",
    "\n",
    "# Build and fit pipeline\n",
    "pipeline = Pipeline(stages=[indexer, tokenizer, stop_words_remover, cv])\n",
    "model = pipeline.fit(comments_with_sentiment)\n",
    "prepared_data = model.transform(comments_with_sentiment)\n",
    "\n",
    "# Split into training and test sets\n",
    "train_data, test_data = prepared_data.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training set size: {train_data.count()}, Test set size: {test_data.count()}\")\n",
    "\n",
    "# Train a simple logistic regression model\n",
    "print(\"Training sentiment classification model...\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Use MulticlassClassificationEvaluator\n",
    "print(\"\\nEvaluating model with MulticlassClassificationEvaluator:\")\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\")\n",
    "\n",
    "# Calculate different metrics\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "\n",
    "print(f\"Weighted Precision: {precision:.4f}\")\n",
    "print(f\"Weighted Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Simple topic modeling similar to the LDA example in the lab\n",
    "print(\"\\nPerforming simple topic modeling on comments...\")\n",
    "\n",
    "# Use the processed features for LDA\n",
    "num_topics = 5\n",
    "lda = LDA(k=num_topics, maxIter=10, featuresCol=\"features\")\n",
    "lda_model = lda.fit(prepared_data)\n",
    "\n",
    "# Extract topics\n",
    "topics = lda_model.describeTopics(3)  # Top 3 terms per topic\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Get topic keywords\n",
    "cv_model = model.stages[3]  # CountVectorizer model\n",
    "vocabulary = cv_model.vocabulary\n",
    "\n",
    "# Map topic indices to actual words\n",
    "def map_indices_to_words(indices):\n",
    "    return [vocabulary[int(idx)] for idx in indices]\n",
    "\n",
    "map_indices_udf = udf(map_indices_to_words, ArrayType(StringType()))\n",
    "topics_with_words = topics.withColumn(\"termWords\", map_indices_udf(col(\"termIndices\")))\n",
    "print(\"\\nTop terms for each topic:\")\n",
    "topics_with_words.select(\"topic\", \"termWords\").show(truncate=False)\n",
    "\n",
    "# FIX: Topic distribution analysis - properly handle DenseVector\n",
    "print(\"\\nCreating topic distribution data...\")\n",
    "# Transform data to get topic distributions\n",
    "transformed_data = lda_model.transform(prepared_data)\n",
    "\n",
    "# Create a function to convert DenseVector to array\n",
    "def vector_to_array(vec):\n",
    "    return vec.toArray().tolist()\n",
    "\n",
    "vector_to_array_udf = udf(vector_to_array, ArrayType(FloatType()))\n",
    "\n",
    "# Convert DenseVector to array\n",
    "transformed_data = transformed_data.withColumn(\"topic_dist_array\", \n",
    "                                              vector_to_array_udf(col(\"topicDistribution\")))\n",
    "\n",
    "# Use array positions to analyze topics\n",
    "print(\"\\nAnalyzing topic weights by sentiment category:\")\n",
    "\n",
    "# Calculate average topic weights per sentiment category\n",
    "summary_data = transformed_data.select(\"sentiment_category\", \"topic_dist_array\")\n",
    "\n",
    "# Calculate average weight for each topic by sentiment\n",
    "topic_avgs = []\n",
    "for topic_idx in range(num_topics):\n",
    "    # Create a selector for this topic's weight\n",
    "    topic_avg = summary_data.withColumn(f\"topic_{topic_idx}_weight\", \n",
    "                                       col(\"topic_dist_array\")[topic_idx])\n",
    "    \n",
    "    # Group by sentiment and calculate average\n",
    "    avg_by_sentiment = topic_avg.groupBy(\"sentiment_category\").agg(\n",
    "        avg(f\"topic_{topic_idx}_weight\").alias(f\"avg_topic_{topic_idx}\")\n",
    "    )\n",
    "    \n",
    "    topic_avgs.append(avg_by_sentiment)\n",
    "\n",
    "# Join the results together\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "if topic_avgs:\n",
    "    joined_avgs = reduce(lambda df1, df2: df1.join(df2, \"sentiment_category\"), topic_avgs)\n",
    "    print(\"\\nAverage topic weights by sentiment category:\")\n",
    "    joined_avgs.show()\n",
    "else:\n",
    "    print(\"No topic averages to display\")\n",
    "\n",
    "# Show examples for each sentiment category\n",
    "print(\"\\nExample comments for each sentiment category:\")\n",
    "for category in [\"Very Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Very Positive\"]:\n",
    "    examples = comments_with_sentiment.filter(col(\"sentiment_category\") == category).limit(2)\n",
    "    examples_count = examples.count()\n",
    "    \n",
    "    if examples_count > 0:\n",
    "        print(f\"\\n--- {category} examples ---\")\n",
    "        for row in examples.collect():\n",
    "            score = row[\"sentiment_score\"]\n",
    "            # Get text and truncate if needed\n",
    "            text = row[body_column]\n",
    "            if len(text) > 100:\n",
    "                text = text[:97] + \"...\"\n",
    "            print(f\"Score: {score:.2f}\")\n",
    "            print(f\"Text: {text}\\n\")\n",
    "    else:\n",
    "        print(f\"\\nNo {category} examples found\")\n",
    "\n",
    "print(\"\\nAnalysis complete!\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48889c1f-091f-49a2-8734-724e4510fa9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìä Sentiment Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d114a59-cbc6-4050-84da-15dcef1b9f47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sentiment_distribution = comments_with_sentiment.groupBy(\"sentiment_category\").count().orderBy(\n",
    "    when(col(\"sentiment_category\") == \"Very Negative\", 1)\n",
    "    .when(col(\"sentiment_category\") == \"Negative\", 2)\n",
    "    .when(col(\"sentiment_category\") == \"Neutral\", 3)\n",
    "    .when(col(\"sentiment_category\") == \"Positive\", 4)\n",
    "    .when(col(\"sentiment_category\") == \"Very Positive\", 5)\n",
    ")\n",
    "sentiment_distribution.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "573c377d-014c-4aa9-9287-d462f56e7c53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ü§ñ Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a66e9e87-4fde-4c2c-b8bb-be8d841e4ba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"sentiment_category\", outputCol=\"label\", handleInvalid=\"skip\")\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"features\", minDF=2.0, vocabSize=10000)\n",
    "pipeline = Pipeline(stages=[indexer, tokenizer, stop_words_remover, cv])\n",
    "model = pipeline.fit(comments_with_sentiment)\n",
    "prepared_data = model.transform(comments_with_sentiment)\n",
    "train_data, test_data = prepared_data.randomSplit([0.8, 0.2], seed=42)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_data)\n",
    "predictions = lr_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40af32b2-b1aa-4815-903b-9a9966cfa717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìù Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89bff593-6ad5-4a9b-8ff4-a996a2d5428e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "label_counts = predictions.groupBy(\"label\").count().collect()\n",
    "prediction_counts = predictions.groupBy(\"prediction\").count().collect()\n",
    "label_prediction_counts = predictions.groupBy(\"label\", \"prediction\").count().collect()\n",
    "label_count_dict = {row[\"label\"]: row[\"count\"] for row in label_counts}\n",
    "prediction_count_dict = {row[\"prediction\"]: row[\"count\"] for row in prediction_counts}\n",
    "label_prediction_dict = {(row[\"label\"], row[\"prediction\"]): row[\"count\"] for row in label_prediction_counts}\n",
    "for label in sorted(label_count_dict.keys()):\n",
    "    tp = label_prediction_dict.get((label, label), 0)\n",
    "    fp = sum(label_prediction_dict.get((other, label), 0) for other in label_count_dict if other != label)\n",
    "    fn = sum(label_prediction_dict.get((label, other), 0) for other in prediction_count_dict if other != label)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    print(f\"Class {label}: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}\")\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\")\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "print(f\"Weighted Precision: {precision:.4f}\\nWeighted Recall: {recall:.4f}\\nF1 Score: {f1:.4f}\\nAccuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "def36bd3-c444-4d21-83e8-9d9eff4591ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üè∑Ô∏è Topic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a4e2b17-80e1-48bf-a1d1-3adec0ba62cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "lda = LDA(k=num_topics, maxIter=10, featuresCol=\"features\")\n",
    "lda_model = lda.fit(prepared_data)\n",
    "topics = lda_model.describeTopics(3)\n",
    "topics.show(truncate=False)\n",
    "cv_model = model.stages[3]\n",
    "vocabulary = cv_model.vocabulary\n",
    "def map_indices_to_words(indices): return [vocabulary[int(idx)] for idx in indices]\n",
    "map_indices_udf = udf(map_indices_to_words, ArrayType(StringType()))\n",
    "topics_with_words = topics.withColumn(\"termWords\", map_indices_udf(col(\"termIndices\")))\n",
    "topics_with_words.select(\"topic\", \"termWords\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3693964d-b5d8-46f1-92cc-e86d3f0935a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîó Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "628e709b-851d-4fa7-862e-3f26e2655db8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if has_issues:\n",
    "    sentiment_by_issue = comments_with_sentiment.groupBy(\"key\").agg(\n",
    "        avg(\"sentiment_score\").alias(\"avg_sentiment\"),\n",
    "        count(\"*\").alias(\"comment_count\")\n",
    "    )\n",
    "    issues_with_sentiment = sentiment_by_issue.join(issues_df, \"key\", \"inner\")\n",
    "    if all(c in issues_df.columns for c in [\"created\", \"resolutiondate\"]):\n",
    "        from pyspark.sql.functions import to_date, datediff\n",
    "        issues_with_time = issues_with_sentiment.withColumn(\n",
    "            \"created_date\", to_date(col(\"created\"))\n",
    "        ).withColumn(\n",
    "            \"resolution_date\", to_date(col(\"resolutiondate\"))\n",
    "        ).withColumn(\n",
    "            \"resolution_days\", when(col(\"resolution_date\").isNotNull() & col(\"created_date\").isNotNull(),\n",
    "                                   datediff(col(\"resolution_date\"), col(\"created_date\"))).otherwise(None)\n",
    "        )\n",
    "        resolved_issues = issues_with_time.filter(col(\"resolution_days\").isNotNull() & (col(\"resolution_days\") >= 0))\n",
    "        resolved_issues.withColumn(\n",
    "            \"sentiment_bucket\",\n",
    "            when(col(\"avg_sentiment\") <= -0.6, \"Very Negative\")\n",
    "            .when(col(\"avg_sentiment\") <= -0.2, \"Negative\")\n",
    "            .when(col(\"avg_sentiment\") <= 0.2, \"Neutral\")\n",
    "            .when(col(\"avg_sentiment\") <= 0.6, \"Positive\")\n",
    "            .otherwise(\"Very Positive\")\n",
    "        ).groupBy(\"sentiment_bucket\").agg(\n",
    "            count(\"*\").alias(\"issue_count\"),\n",
    "            avg(\"resolution_days\").alias(\"avg_resolution_days\"),\n",
    "            min(\"resolution_days\").alias(\"min_resolution_days\"),\n",
    "            max(\"resolution_days\").alias(\"max_resolution_days\")\n",
    "        ).orderBy(\n",
    "            when(col(\"sentiment_bucket\") == \"Very Negative\", 1)\n",
    "            .when(col(\"sentiment_bucket\") == \"Negative\", 2)\n",
    "            .when(col(\"sentiment_bucket\") == \"Neutral\", 3)\n",
    "            .when(col(\"sentiment_bucket\") == \"Positive\", 4)\n",
    "            .when(col(\"sentiment_bucket\") == \"Very Positive\", 5)\n",
    "        ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "325fcfb1-12a8-4b72-8bbe-e8c03a44bd70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìù Example Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a35d408-a4c6-4126-8d8c-625e20d6a97d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for category in [\"Very Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Very Positive\"]:\n",
    "    examples = comments_with_sentiment.filter(col(\"sentiment_category\") == category).limit(2)\n",
    "    if examples.count() > 0:\n",
    "        print(f\"\\n--- {category} examples ---\")\n",
    "        for row in examples.collect():\n",
    "            text = row[body_column]\n",
    "            text = (text[:97] + \"...\") if len(text) > 100 else text\n",
    "            print(f\"Score: {row['sentiment_score']:.2f}\\nText: {text}\\n\")\n",
    "    else:\n",
    "        print(f\"\\nNo {category} examples found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84343db5-88ba-4901-b94b-b5fed44de82a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a525b04e-d42b-4b6e-aca5-3aa48389a2c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Comment_Sentiment",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}