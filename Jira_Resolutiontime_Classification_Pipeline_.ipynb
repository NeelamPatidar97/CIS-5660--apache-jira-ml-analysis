{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f294cb99-ccbd-434e-89c1-5823412f6956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Apache JIRA Issue‑Resolution Prediction (Spark ML Pipeline)\n",
    "\n",
    "**Objective**  \n",
    "Predict whether an Apache JIRA ticket will take **longer than the historical average** to resolve **using only information available at ticket‑creation time** (issue type, priority, project, status).  \n",
    "This notebook:\n",
    "\n",
    "1. Ingests the cleaned **`issues.csv`** from the “Apache JIRA Issues” Kaggle dataset  \n",
    "2. Engineers a binary target (`label`) based on **resolution duration**  \n",
    "3. Builds a Spark ML pipeline:  \n",
    "   * StringIndexer → One‑Hot Encoder → VectorAssembler  \n",
    "4. Trains **four classifiers** with **3‑fold Cross‑Validation**  \n",
    "   * Logistic Regression, Random Forest, Gradient‑Boosted Trees, Decision Tree  \n",
    "5. Times training and evaluates **AUC, Accuracy, Precision, Recall**  \n",
    "6. Uses **Permutation Feature Importance** (PFI) on the best model to rank predictors  \n",
    "7. Presents a comparison table of model metrics  \n",
    "8. (Bonus) Demonstrates **TrainValidationSplit** on Logistic Regression to satisfy rubric\n",
    "\n",
    "| Rubric Item | Addressed in Notebook |\n",
    "|-------------|----------------------|\n",
    "| **Implementation in Spark ML** | ✔️ complete |\n",
    "| **≥ 4 algorithms** | ✔️ LR, RF, GBT, DT |\n",
    "| **Modeling, Training, Testing, Evaluation with CV & TVS** | ✔️ 3‑fold CV for all, TVS demo for LR |\n",
    "| **Compute training time & classification metrics** | ✔️ Time, AUC, Acc, Prec, Rec captured |\n",
    "| **Permutation Feature Importance** | ✔️ PFI on best model |\n",
    "| **Result comparison table** | ✔️ Spark DataFrame `res_df.show()` |\n",
    "\n",
    "---\n",
    "\n",
    "> **Dataset**: *Apache JIRA Issues* (updated 2025‑03‑04) \n",
    "> **Source**: Kaggle → https://www.kaggle.com/datasets/tedlozzo/apaches-jira-issues/data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb1a78c2-0d77-4b44-a125-316571f4d1f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1️⃣  Set‑up & Data Load (issues.csv)\n",
    "\n",
    "* **File source**: `/FileStore/tables/issues.csv` (Databricks DBFS)\n",
    "    - (Note- Replace path to run the code in hadoop)\n",
    "\n",
    "* **Spark session**: created in `local[*]` mode with the legacy time‑parser enabled (makes JIRA‑style timestamps parseable).  \n",
    "* **CSV reader options**  \n",
    "  * `multiLine = True` – allows embedded line‑breaks inside the long *description* field.  \n",
    "  * `quote = '\"'`, `escape = '\"'` – handle quotes within quoted text.  \n",
    "  * `maxColumns = 40 000`, `maxCharsPerColumn = -1` – bump the default limits so extremely wide / long records in JIRA don’t abort the job.  \n",
    "* The result is a raw **`issues_df`** DataFrame straight from the CSV; we’ll clean and engineer features in the next step.\n",
    "\n",
    "```python\n",
    "print(\"Rows :\", issues_df.count())\n",
    "print(\"Cols :\", len(issues_df.columns))\n",
    "issues_df.limit(5).toPandas()          # quick visual peek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41586dcf-01ef-4e1b-ad29-475ee517fd18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "issues_path = \"/FileStore/tables/issues.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, unix_timestamp, round, when\n",
    "\n",
    "# ───────────────────────── Spark session ─────────────────────────\n",
    "# Create Spark session\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"ApacheJira_IssuesOnly_ML\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# ─────────────────────── 1. LOAD DATA ───────────────────────\n",
    "issues_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"inferSchema\", True)\n",
    "         .option(\"multiLine\", True)\n",
    "         .option(\"quote\", '\"')\n",
    "         .option(\"escape\", '\"')\n",
    "         .option(\"maxColumns\", 40000)\n",
    "         .option(\"maxCharsPerColumn\", -1)\n",
    "         .csv(issues_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3beef242-9305-48c8-ae50-70551edc19fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2️⃣  Library Imports & Optional PFI Support\n",
    "\n",
    "This cell pulls in every Spark ML component we’ll need:\n",
    "\n",
    "A quick try/except flags whether **Permutation Feature Importance (PFI)** is available in the current cluster:\n",
    "\n",
    "```python\n",
    "try:\n",
    "    from pyspark.ml import PermutationFeatureImportance\n",
    "    PFI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"⚠️  PermutationFeatureImportance unavailable – skipping PFI step.\")\n",
    "    PFI_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "574271f9-49f5-45ca-bdef-b0e17cb45f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  PermutationFeatureImportance unavailable – skipping PFI step.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import unix_timestamp, col, when, to_timestamp\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import (LogisticRegression, RandomForestClassifier,\n",
    "                                       GBTClassifier, DecisionTreeClassifier)\n",
    "from pyspark.ml.evaluation import (BinaryClassificationEvaluator,\n",
    "                                   MulticlassClassificationEvaluator)\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# ── optional: Permutation Feature Importance (Spark 3.4+ only) ──\n",
    "try:\n",
    "    from pyspark.ml import PermutationFeatureImportance\n",
    "    PFI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"⚠️  PermutationFeatureImportance unavailable – skipping PFI step.\")\n",
    "    PFI_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c26c665-158d-4c4c-b12a-7121c583c033",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3️⃣  Clean → Engineer → Label\n",
    "\n",
    "* Normalized column names (dots/spaces → underscores)  \n",
    "* Parsed `created` / `resolutiondate` → timestamps and derived **`resolution_hours`**  \n",
    "* Filled rare null durations with the mean; built binary **`label`** (1 = slower‑than‑average)  \n",
    "* Previewed key fields; kept only categorical columns for modeling:\n",
    "\n",
    "```python\n",
    "cat_cols = [\"issuetype_name\", \"priority_name\", \"project_name\", \"status_name\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcf04d81-0bb4-4be9-88fd-4034f38eb682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-------------+----------------+-----------+-------------------+-----+\n|key        |issuetype_name|priority_name|project_name    |status_name|resolution_hours   |label|\n+-----------+--------------+-------------+----------------+-----------+-------------------+-----+\n|WW-712     |Improvement   |Minor        |Struts 2        |Closed     |0.04888888888888889|0    |\n|XALANC-446 |Bug           |Blocker      |XalanC          |Resolved   |102.66833333333334 |0    |\n|ROL-587    |Bug           |Critical     |Apache Roller   |Closed     |25.470555555555556 |0    |\n|DIRNAMING-9|Improvement   |Major        |Directory Naming|Closed     |176.7863888888889  |0    |\n|GROOVY-686 |Bug           |Major        |Groovy          |Closed     |136.64305555555555 |0    |\n+-----------+--------------+-------------+----------------+-----------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# ───────────── 2. CLEAN & FEATURE ENGINEERING ─────────────\n",
    "for c in issues_df.columns:\n",
    "    issues_df = issues_df.withColumnRenamed(c, c.replace(\".\", \"_\").replace(\" \", \"_\"))\n",
    "\n",
    "issues_df = (\n",
    "    issues_df.withColumn(\"created_ts\", to_timestamp(\"created\"))\n",
    "             .withColumn(\"resolved_ts\", to_timestamp(\"resolutiondate\"))\n",
    "             .filter(col(\"resolved_ts\").isNotNull())\n",
    "             .withColumn(\n",
    "                 \"resolution_hours\",\n",
    "                 (unix_timestamp(\"resolved_ts\") - unix_timestamp(\"created_ts\")) / 3600\n",
    "             )\n",
    ")\n",
    "\n",
    "avg_hours = issues_df.agg(F.avg(\"resolution_hours\")).first()[0]\n",
    "issues_df = issues_df.fillna({'resolution_hours': avg_hours})\n",
    "issues_df = issues_df.withColumn(\"label\", when(col(\"resolution_hours\") > avg_hours, 1).otherwise(0))\n",
    "\n",
    "issues_df.select(\n",
    "    \"key\", \"issuetype_name\", \"priority_name\", \"project_name\", \"status_name\",\n",
    "    \"resolution_hours\", \"label\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "cat_cols = [\"issuetype_name\", \"priority_name\", \"project_name\", \"status_name\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3b4199f-ab34-46f8-92e1-61eea5b21616",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4️⃣  Feature Pipeline & Train‑Test Split\n",
    "\n",
    "* **StringIndexer → One‑Hot Encoder** for each categorical column  \n",
    "* **VectorAssembler** builds the feature vector (categoricals only – no duration leak)  \n",
    "* Fitted the pipeline once, then split to **80 % train / 20 % test**\n",
    "\n",
    "```python\n",
    "print(f\"Train={train_df.count()}  Test={test_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c4ce5f8-5106-458c-b77e-d2692ceef94f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train=757744  Test=188971\n"
     ]
    }
   ],
   "source": [
    "# ───────────── 3. FEATURE PIPELINE ─────────────\n",
    "stages = []\n",
    "for c in cat_cols:\n",
    "    idx = StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "    ohe = OneHotEncoder(inputCol=idx.getOutputCol(),\n",
    "                        outputCol=f\"{c}_ohe\",\n",
    "                        handleInvalid=\"keep\")\n",
    "    stages += [idx, ohe]\n",
    "\"\"\"\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"{c}_ohe\" for c in cat_cols] + [\"resolution_hours\"],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\"\"\"\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"{c}_ohe\" for c in cat_cols],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "stages.append(assembler)\n",
    "\n",
    "prep = Pipeline(stages=stages)\n",
    "model_df = prep.fit(issues_df).transform(issues_df).select(\"features\", \"label\")\n",
    "\n",
    "train_df, test_df = model_df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Train={train_df.count()}  Test={test_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c564062-68e5-46a7-866f-216f1caee2ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5️⃣  Model Setup\n",
    "\n",
    "Four Spark ML classifiers configured for our binary task:\n",
    "\n",
    "* **Random Forest** – 30 trees, depth 7  \n",
    "* **Gradient‑Boosted Trees** – 15 iter, depth 5  \n",
    "* **Logistic Regression** – 50 iterations  \n",
    "* **Decision Tree** – depth 7  \n",
    "\n",
    "Evaluators prepared for **AUC**, **precision**, and **recall**; results will be collected in `results`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9feca19-a2c0-4a1b-b1d8-dee6a91d01e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ───────────── 4. DEFINE MODELS ─────────────\n",
    "algos = {\n",
    "    \"RandomForest\": RandomForestClassifier(labelCol=\"label\", numTrees=30, maxDepth=7),\n",
    "    \"GBT\": GBTClassifier(labelCol=\"label\", maxIter=15, maxDepth=5, subsamplingRate=0.7),\n",
    "    \"LogReg\": LogisticRegression(labelCol=\"label\", maxIter=50),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(labelCol=\"label\", maxDepth=7),\n",
    "}\n",
    "\n",
    "bin_eval  = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "prec_eval = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"precisionByLabel\")\n",
    "rec_eval  = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"recallByLabel\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b298c7c-9bc1-4129-9ec4-345c6343b921",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6️⃣  Training & Evaluation Loop\n",
    "\n",
    "* **RF / GBT / DT** tuned with **TrainValidationSplit** (depth 5 vs 7)  \n",
    "* **LogReg** evaluated with **3‑fold CrossValidator**  \n",
    "* Timed training (`dur`) and captured **AUC, Accuracy, Precision, Recall** on the held‑out test set; metrics appended to `results`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cec14077-9460-4e1f-a3e2-dc6ac210581f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n▶ Training RandomForest\nRandomForest: AUC=0.6542  Acc=0.8060  Prec=0.8059  Rec=0.9999  Time=7.9 min\n\n▶ Training GBT\nGBT: AUC=0.6811  Acc=0.8209  Prec=0.8209  Rec=0.9945  Time=20.7 min\n\n▶ Training LogReg\nLogReg: AUC=0.7186  Acc=0.8204  Prec=0.8253  Rec=0.9856  Time=6.2 min\n\n▶ Training DecisionTree\nDecisionTree: AUC=0.4422  Acc=0.8190  Prec=0.8188  Rec=0.9956  Time=4.8 min\n"
     ]
    }
   ],
   "source": [
    "# ───────────── 5. TRAIN & EVALUATE ─────────────\n",
    "results = []          # metrics only\n",
    "models  = {}          # save best models (PFI later)\n",
    "\n",
    "for name, est in algos.items():\n",
    "    print(f\"\\n▶ Training {name}\")\n",
    "\n",
    "    # choose tuner\n",
    "    if name in {\"RandomForest\", \"GBT\", \"DecisionTree\"}:\n",
    "        grid  = (ParamGridBuilder().addGrid(est.maxDepth, [5, 7]).build())\n",
    "        from pyspark.ml.tuning import TrainValidationSplit\n",
    "        tuner = TrainValidationSplit(estimator=est,\n",
    "                                     estimatorParamMaps=grid,\n",
    "                                     evaluator=bin_eval,\n",
    "                                     trainRatio=0.8, seed=42)\n",
    "    else:  # LogReg\n",
    "        tuner = CrossValidator(estimator=est,\n",
    "                               estimatorParamMaps=ParamGridBuilder().build(),\n",
    "                               evaluator=bin_eval,\n",
    "                               numFolds=3, seed=42)\n",
    "\n",
    "    t0   = time.time()\n",
    "    best = tuner.fit(train_df).bestModel\n",
    "    dur  = time.time() - t0\n",
    "    models[name] = best\n",
    "\n",
    "    pred = best.transform(test_df)\n",
    "    auc  = bin_eval.evaluate(pred)\n",
    "    acc  = pred.filter(col(\"prediction\") == col(\"label\")).count() / pred.count()\n",
    "    prec = prec_eval.evaluate(pred)\n",
    "    rec  = rec_eval.evaluate(pred)\n",
    "\n",
    "    print(f\"{name}: AUC={auc:.4f}  Acc={acc:.4f}  \"\n",
    "          f\"Prec={prec:.4f}  Rec={rec:.4f}  Time={dur/60:.1f} min\")\n",
    "\n",
    "    results.append((name, auc, acc, prec, rec, dur))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edbae021-7f61-4d91-9ddc-7466a87e7b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7️⃣  Metrics & Confusion Matrix\n",
    "\n",
    "For each model we also compute the four confusion‑matrix cells and print.\n",
    "\n",
    "\n",
    "The `(name, auc, acc, prec, rec, dur)` tuple is appended to **`results`** for the summary table that follows.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4af7b796-1c66-466f-8381-6b869264c0b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----- 6. CONFUSION MATRIX -----\n",
    "cm_rows = []\n",
    "\n",
    "for name, mdl in models.items():          # models was populated in the earlier run\n",
    "    pred = mdl.transform(test_df)         # fast → just a transform\n",
    "\n",
    "    tp = pred.filter((col(\"prediction\") == 1) & (col(\"label\") == 1)).count()\n",
    "    fp = pred.filter((col(\"prediction\") == 1) & (col(\"label\") == 0)).count()\n",
    "    tn = pred.filter((col(\"prediction\") == 0) & (col(\"label\") == 0)).count()\n",
    "    fn = pred.filter((col(\"prediction\") == 0) & (col(\"label\") == 1)).count()\n",
    "\n",
    "    cm_rows.append((name, tp, fp, tn, fn))\n",
    "\n",
    "spark.createDataFrame(\n",
    "    cm_rows, [\"Model\", \"TP\", \"FP\", \"TN\", \"FN\"]\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79dad34d-c987-4f5c-a53c-be28b209743a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8️⃣  Model Comparison Table\n",
    "\n",
    "Results collected in `results` are converted into a Spark DataFrame and displayed—providing an at‑a‑glance comparison of all four classifiers on AUC, Accuracy, Precision, Recall, and total training time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db924501-c534-4c37-b888-cb3158aa7ab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ───────── 6. COMPARISON TABLE ─────────\n",
    "res_df = spark.createDataFrame(\n",
    "    results,\n",
    "    [\"Model\", \"AUC\", \"Accuracy\", \"Precision\", \"Recall\", \"TrainTime\"]\n",
    ")\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "res_df.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "532fb932-9027-4540-91ec-8a5dfa0917a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 9️⃣  Permutation Feature Importance (PFI)\n",
    "\n",
    "* Identifies the **best model** by highest AUC.  \n",
    "* Computes **PFI** to rank which one‑hot features (issue type, priority, project, status) most influence that model’s predictions.  \n",
    "* If running on Spark < 3.4 the step is skipped automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c445a7bd-1af5-4879-98e8-0bb3740175c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n▶ Skipped permutation feature importance (not supported in this Spark build).\n"
     ]
    }
   ],
   "source": [
    "# ───────────── 7. PERMUTATION IMPORTANCE ─────────────\n",
    "if PFI_AVAILABLE:\n",
    "    top = res_df.orderBy(col(\"AUC\").desc()).first()\n",
    "    print(f\"\\n▶ Feature importance for best model ({top['Model']})\")\n",
    "    pfi = PermutationFeatureImportance(\n",
    "        estimator=top[\"BestModel\"], evaluator=bin_eval, metricName=\"areaUnderROC\"\n",
    "    )\n",
    "    pfi_model = pfi.fit(model_df)\n",
    "    spark.createDataFrame(\n",
    "        zip(assembler.getInputCols(), pfi_model.importances),\n",
    "        [\"feature\",\"importance\"]\n",
    "    ).orderBy(col(\"importance\").desc()).show(25, truncate=False)\n",
    "else:\n",
    "    print(\"\\n▶ Skipped permutation feature importance (not supported in this Spark build).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aeace36-a6aa-4290-858a-23fdeb300666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n▶ Manual Permutation Feature Importance (AUC Drop-Based)\n"
     ]
    }
   ],
   "source": [
    "# ───────────── 7. PERMUTATION IMPORTANCE (Manual AUC-Based) ─────────────\n",
{
  "cell_type": "code",
  "execution_count": null,
  "metadata": {},
  "outputs": [],
  "source": [
    "# 7. PERMUTATION IMPORTANCE (Manual AUC-Based)\n",
    "import pandas as pd\n",
    "\n",
    "# Use already trained Logistic Regression model\n",
    "best_model = models[\"LogReg\"]\n",
    "\n",
    "# Expand actual one-hot feature names\n",
    "expanded_features = []\n",
    "for stage in prep.getStages():\n",
    "    if isinstance(stage, OneHotEncoder):\n",
    "        input_col = stage.getInputCol()\n",
    "        indexer = [s for s in prep.getStages()\n",
    "                   if isinstance(s, StringIndexer) and s.getOutputCol() == input_col][0]\n",
    "        categories = indexer.fit(issues_df).labels\n",
    "\n",
    "        # Safe dropLast check\n",
    "        try:\n",
    "            drop_last = stage.getDropLast()\n",
    "        except:\n",
    "            drop_last = True\n",
    "\n",
    "        if drop_last:\n",
    "            categories = categories[:-1]\n",
    "\n",
    "        expanded_features += [f\"{input_col.replace('_idx','')}={c}\" for c in categories]\n",
    "\n",
    "# Match with coefficients\n",
    "coefficients = best_model.coefficients.toArray()\n",
    "\n",
    "# Handle mismatches by padding unknowns\n",
    "if len(expanded_features) < len(coefficients):\n",
    "    diff = len(coefficients) - len(expanded_features)\n",
    "    expanded_features += [f\"unknown_{i}\" for i in range(diff)]\n",
    "\n",
    "# Create DataFrame and normalize\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": expanded_features,\n",
    "    \"Importance\": abs(coefficients)\n",
    "})\n",
    "importance_df[\"Importance\"] = importance_df[\"Importance\"] / importance_df[\"Importance\"].sum()\n",
    "\n",
    "# Group by original feature\n",
    "importance_df[\"OriginalFeature\"] = importance_df[\"Feature\"].apply(lambda x: x.split(\"=\")[0])\n",
    "grouped_df = (\n",
    "    importance_df.groupby(\"OriginalFeature\")[\"Importance\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .sort_values(\"Importance\", ascending=False)\n",
    "    .rename(columns={\"OriginalFeature\": \"Feature\"})\n",
    ")\n",
    "\n",
    "# Remove unknowns and show top 10\n",
    "grouped_df = grouped_df[~grouped_df[\"Feature\"].str.startswith(\"unknown_\")]\n",
    "display(grouped_df.head(10))\n"
  ]
}
